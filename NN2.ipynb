{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6701ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit  # für Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25116dd",
   "metadata": {},
   "source": [
    "<img src=\"image-2.png\" alt=\"Bildbeschreibung\" width=\"40%\"/>\n",
    "\n",
    "\n",
    "### Forward Method: \n",
    "<img src=\"image.png\" alt=\"Bildbeschreibung\" width=\"40%\"/>\n",
    "\n",
    "\n",
    "### Gradient Calculation\n",
    "<img src=\"image-3.png\" alt=\"Bildbeschreibung\" width=\"40%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49272603",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size = 8, hidden1_size=100, hidden2_size = 50, output_size = 1, lr=0.1):\n",
    "        \n",
    "        # Learning rate\n",
    "        self.lr = lr\n",
    "        # Weights and biases\n",
    "        # Input -> Hidden1\n",
    "        self.w0 = np.random.randn(hidden1_size, input_size) * 0.01\n",
    "        self.b0 = np.zeros((hidden1_size, 1))\n",
    "\n",
    "        # # Hidden1 → hidden2\n",
    "        self.w1 = np.random.randn(hidden2_size, hidden1_size) * 0.01\n",
    "        self.b1 = np.zeros((hidden2_size, 1))\n",
    "\n",
    "\n",
    "        # Hidden2 -> Output\n",
    "        self.w2 = np.random.randn(output_size, hidden2_size) * 0.01\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "\n",
    "        # Activation values\n",
    "        self.a0 = None\n",
    "        self.a1 = None\n",
    "        self.a2 = None\n",
    "\n",
    "        \n",
    "    # Sigmoid function\n",
    "    def sigmoid(self, x):\n",
    "        return expit(x)  \n",
    "    \n",
    "    # Derivative of sigmoid function\n",
    "    def sigmoid_deriv(self, a):\n",
    "        return a * (1 - a) \n",
    "    \n",
    "    # Loss function\n",
    "    def loss(self, y_true, y_pred):\n",
    "        #m = y_true.shape[1] # m = number of samples\n",
    "        loss = 0.5 * np.sum((y_true - y_pred)**2)  \n",
    "        return loss\n",
    "\n",
    "    # Derivative of loss function    \n",
    "    def loss_deriv(self, y_true, y_pred):\n",
    "        #m = y_true.shape[1]\n",
    "        return (y_pred - y_true) # / m\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, X):\n",
    "        self.X = X  \n",
    "\n",
    "        ### 1. Hidden Layer\n",
    "        # X.T: (input_size, m)\n",
    "        # z0 = W0 @ X_T + b0 \n",
    "        # a0 = sig(z0)\n",
    "        self.a0 = self.sigmoid(self.w0 @ X.T + self.b0)  # a0.shape (hidden_size, m) -> (100, 614)\n",
    "        \n",
    "        ### 2. hidden layer \n",
    "        # z1 = W1 @ a0 + b1\n",
    "        # a1 = sig(z1)\n",
    "        self.a1 = self.sigmoid(self.w1 @ self.a0 + self.b1)  # a1.shape: (output_size, m) -> (2, 614)\n",
    "\n",
    "        # output Layer\n",
    "        self.a2 = self.sigmoid(self.w2 @ self.a1 + self.b2) \n",
    "        \n",
    "        return self.a2\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, X, y_true):\n",
    "\n",
    "        Y = y_true.T      # (output_size, m)\n",
    "        m = Y.shape[1]    # samples\n",
    "\n",
    "        y_true = y_true.T \n",
    "\n",
    "        ### OUTPUT LAYER\n",
    "        # delta2 = self.a1 - y_true.T # optional this simple calculation\n",
    "        delta2 = self.loss_deriv(Y, self.a2) * self.sigmoid_deriv(self.a2)\n",
    "\n",
    "        # Gradients for w2, b2: \n",
    "        dw2 = (delta2 @ self.a1.T) / m  \n",
    "        db2 = np.sum(delta2, axis=1, keepdims=True) / m\n",
    "\n",
    "        ### HIDDEN LAYER 2\n",
    "        # (weights output layer x error output layer) * derived activation \n",
    "        delta1 = (self.w2.T @ delta2) * self.sigmoid_deriv(self.a1)\n",
    "\n",
    "        # Gradients for w1, b1:\n",
    "        # dw1: error in hidden layer 1 * outputs from hiddenlayer 2 (a2)\n",
    "        dw1 = delta1 @ self.a0.T / m\n",
    "        db1 = np.sum(delta1, axis=1, keepdims=True) / m # sum errors in current layer\n",
    "\n",
    "        ### HIDDEN LAYER 1\n",
    "        # Error back to the previous layer \n",
    "        delta0 = (self.w1.T @ delta1) * self.sigmoid_deriv(self.a0)\n",
    "\n",
    "        # Gradients for w0, b0: \n",
    "        dw0 = (delta0 @ self.X) / m\n",
    "        db0 = np.sum(delta0, axis=1, keepdims=True) / m\n",
    "\n",
    "        # Update\n",
    "        self.w2 -= self.lr * dw2\n",
    "        self.b2 -= self.lr * db2\n",
    "        self.w1 -= self.lr * dw1\n",
    "        self.b1 -= self.lr * db1\n",
    "        self.w0 -= self.lr * dw0\n",
    "        self.b0 -= self.lr * db0\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # Get the prediciton for current wheights\n",
    "        y_pred = self.forward(X)\n",
    "\n",
    "        # Compute the loss \n",
    "        current_loss = self.loss(y.T, y_pred)\n",
    "\n",
    "        # Update the weights and biases\n",
    "        self.backward(X, y)\n",
    "\n",
    "        return current_loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        a1 = self.forward(X)\n",
    "        return a1.T\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2de300",
   "metadata": {},
   "source": [
    "### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4303bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv file with pandas\n",
    "df = pd.read_csv('data/diabetes.csv')\n",
    "\n",
    "# drop all na values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89d15fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# head of the dataset\n",
    "#print(df.head())\n",
    "# shape of the dataset\n",
    "#print(df.shape)\n",
    "\n",
    "# one hot encoding of the target variable\n",
    "X = df.drop('Outcome', axis=1).values  # shape: (n_samples, 8)\n",
    "y = df['Outcome'].values.reshape(-1, 1)  # shape: (n_samples, 1)\n",
    "\n",
    "# Skalieren der Eingabedaten\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# One-Hot-Encoding der Zielvariable\n",
    "encoder = OneHotEncoder()\n",
    "y_oh = encoder.fit_transform(y).toarray()\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_oh, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae0bea4",
   "metadata": {},
   "source": [
    "### Train the Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb83b268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 152.4537\n",
      "Epoch 10: Loss = 139.5362\n",
      "Epoch 20: Loss = 139.1284\n",
      "Epoch 30: Loss = 139.1103\n",
      "Epoch 40: Loss = 139.1094\n",
      "Epoch 50: Loss = 139.1093\n",
      "Epoch 60: Loss = 139.1092\n",
      "Epoch 70: Loss = 139.1091\n",
      "Epoch 80: Loss = 139.1091\n",
      "Epoch 90: Loss = 139.1090\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(input_size=8, hidden1_size=100, hidden2_size = 50, output_size=2, lr=0.2)\n",
    "\n",
    "# X_train: (n_samples, 784), y_train_oh: (n_samples, 10)\n",
    "epochs = 100\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    loss = model.train(X_train, y_train)\n",
    "    losses.append(loss)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96dacc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 139.1089\n",
      "Epoch 1000: Loss = 139.1021\n",
      "Epoch 2000: Loss = 139.0918\n",
      "Epoch 3000: Loss = 139.0723\n",
      "Epoch 4000: Loss = 139.0293\n",
      "Epoch 5000: Loss = 138.9134\n",
      "Epoch 6000: Loss = 138.4191\n",
      "Epoch 7000: Loss = 130.6040\n",
      "Epoch 8000: Loss = 94.8590\n",
      "Epoch 9000: Loss = 92.7608\n",
      "Epoch 10000: Loss = 92.6135\n",
      "Epoch 11000: Loss = 92.5477\n",
      "Epoch 12000: Loss = 92.4853\n",
      "Epoch 13000: Loss = 92.4053\n",
      "Epoch 14000: Loss = 92.2789\n",
      "Epoch 15000: Loss = 92.0689\n",
      "Epoch 16000: Loss = 91.7504\n",
      "Epoch 17000: Loss = 91.3150\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m max_epochs = \u001b[32m10000000\u001b[39m      \n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epochs):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     loss = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     losses.append(loss)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m epoch % \u001b[32m1000\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 114\u001b[39m, in \u001b[36mNeuralNetwork.train\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    111\u001b[39m current_loss = \u001b[38;5;28mself\u001b[39m.loss(y.T, y_pred)\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Update the weights and biases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m current_loss\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mNeuralNetwork.backward\u001b[39m\u001b[34m(self, X, y_true)\u001b[39m\n\u001b[32m     88\u001b[39m db1 = np.sum(delta1, axis=\u001b[32m1\u001b[39m, keepdims=\u001b[38;5;28;01mTrue\u001b[39;00m) / m \u001b[38;5;66;03m# sum errors in current layer\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m### HIDDEN LAYER 1\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Error back to the previous layer \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m delta0 = (\u001b[38;5;28mself\u001b[39m.w1.T @ delta1) * \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msigmoid_deriv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43ma0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Gradients for w0, b0: \u001b[39;00m\n\u001b[32m     95\u001b[39m dw0 = (delta0 @ \u001b[38;5;28mself\u001b[39m.X) / m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mNeuralNetwork.sigmoid_deriv\u001b[39m\u001b[34m(self, a)\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m expit(x)  \n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Derivative of sigmoid function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msigmoid_deriv\u001b[39m(\u001b[38;5;28mself\u001b[39m, a):\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m a * (\u001b[32m1\u001b[39m - a) \n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Loss function\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "tolerance = 1e-4   \n",
    "patience = 200             \n",
    "best_loss = float('inf')  \n",
    "no_improvement = 0      \n",
    "max_epochs = 10000000      \n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    loss = model.train(X_train, y_train)\n",
    "    losses.append(loss)\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "\n",
    "    #\n",
    "    if best_loss - loss > tolerance:\n",
    "        best_loss = loss\n",
    "        no_improvement = 0\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "\n",
    "    # \n",
    "    if no_improvement >= patience:\n",
    "        print(f\"\\n🛑 Stop after erpch: {epoch}. no decresment after {patience} epochs\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dc022f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7662\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict \n",
    "y_pred = model.predict(X_test)\n",
    "# Convert predictions to binary\n",
    "y_pred_binary = np.argmax(y_pred, axis=1)\n",
    "y_test_binary = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred_binary == y_test_binary)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# confussion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "cm = confusion_matrix(y_test_binary, y_pred_binary)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca61a95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pytorch \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb878759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 12.9420\n",
      "Epoch 10: Loss = 10.5729\n",
      "Epoch 20: Loss = 9.9717\n",
      "Epoch 30: Loss = 9.3084\n",
      "Epoch 40: Loss = 9.2759\n",
      "Epoch 50: Loss = 9.3603\n",
      "Epoch 60: Loss = 9.1059\n",
      "Epoch 70: Loss = 9.4952\n",
      "Epoch 80: Loss = 9.3354\n",
      "Epoch 90: Loss = 9.2253\n",
      "Accuracy: 0.7662\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Definition des neuronalen Netzes\n",
    "class NeuralNetworkPyTorch(nn.Module):\n",
    "    def __init__(self, input_size=8, hidden1_size=100, hidden2_size=50, output_size=2):\n",
    "        super(NeuralNetworkPyTorch, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden1_size)\n",
    "        self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.fc3 = nn.Linear(hidden2_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)  # Letzte Schicht ohne Aktivierungsfunktion (für CrossEntropyLoss)\n",
    "        return x\n",
    "\n",
    "# Hyperparameter\n",
    "input_size = 8\n",
    "hidden1_size = 100\n",
    "hidden2_size = 50\n",
    "output_size = 2\n",
    "lr = 0.2\n",
    "epochs = 100\n",
    "\n",
    "# Daten in Torch-Tensoren umwandeln\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Dataset und DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Modell, Loss und Optimizer\n",
    "model = NeuralNetworkPyTorch(input_size, hidden1_size, hidden2_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, torch.argmax(batch_y, dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {epoch_loss:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    y_pred_classes = torch.argmax(y_pred, dim=1).numpy()\n",
    "    y_test_classes = torch.argmax(y_test_tensor, dim=1).numpy()\n",
    "    accuracy = (y_pred_classes == y_test_classes).mean()\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
